<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Diary:ae2f</title>
</head>
<body>
    <h1><s>TODO</s> Diary</h1>
    <p>Newer will be on top.</p>

    <h2>Fri Jan 31 02:42:55 KST 2025</h2>
    <p>
        The template of the project has been changed. <br/>
        Existing libraries's current file needs to be changed []
    </p>
    <p>
        <a href="https://github.com/ae2f/Core-Template/blob/main/cmake/Core.cmake">This is the changing file.</a>
    </p>

    <h2>Fri Jan 31 01:43:46 KST 2025</h2>
    <p>
        Alternating python script to shell went successfull. (seemingly) <br/>
        Future dependency (python) will be removed from building library 'ae2f::CL-Core'.
    </p>
    <p>
        Click here to get from <a href="https://github.com/ae2f/CL-Core/releases/tag/Rel-v4.10.14">4.10.14</a>.
    </p>

    <h2>30.Jan.2025</h2>
    <p>
        The problem of MLP test of learning XOR was not actually the Training part, but the Predict part. <br/>
        Problem(solved) is visible <a href="https://github.com/ae2f/CL-Ann/commit/52f48472f912089fc82ad48b84f98ee062c56ca8#diff-380c9639b6e1e1333efd23570a7c5ba7a1fecf06363540a5b77b5b1c6a2ef436L134">here(Line 134, src/Slp.c).</a>
    </p>
    <p>
        The ae2fCL_AnnMlpPredict works as following: <br/>
        <ol>
            <li>Perform ae2fCL_AnnSlpPredict for each element of the List of Mlp.</li>
            <li>Previous output will be considered as next input.</li>
        </ol>

        Problem was mistaking on indexing the [buffobj]. <br/>
        The variable was to store the previous output, but the sector of output to write and input to read has been separated. <br/>
        It resulted the situation that buffobj has been constant as value at first, and that caused malfunctioning of Prediction.
    </p>

    <h2>24.Jan.2025</h2>
    <p>
        Don't know why though predict function from mlp is returning only zero. <br/>
        Perhaps need some unit test for SLP and MSP Predict itself?
    </p>

    <h2>23.Jan.2025</h2>
    <p>
        The pseudo code for MLP is now complete under python. <br>
        This is how I've implemented. <br/><br/>
        
        Comments may have written in Korean.
    </p>
    <fieldset>
    <pre>
import random
import math

# 단층 퍼셉트론 클래스
# 하나의 output을 반환한다
class Perceptron:
    def __init__(self, input_size):
        self.weights = [random.uniform(-1, 1) for _ in range(input_size)]
        self.bias = random.uniform(-1, 1)
    
    def activation(self, x):
        return 1 / (1 + math.exp(-x))
    
    def activation_derivative(self, output):
        return output * (1 - output)
    
    def forward(self, inputs):
        total = sum(w * i for w, i in zip(self.weights, inputs)) + self.bias
        return self.activation(total)
    
    def compute_delta(self, output, target):
        return (target - output) * self.activation_derivative(output)
    
    def train(self, inputs, target, LEARNING_RATE):
        output = self.forward(inputs)
        delta = self.compute_delta(output, target)
        for i in range(len(self.weights)):
            self.weights[i] += LEARNING_RATE * delta * inputs[i]
        self.bias += LEARNING_RATE * delta

# 다중 출력 단층 퍼셉트론 (SLP) 클래스
class SLP:
    def __init__(self, input_size, output_size):
        self.perceptrons = [Perceptron(input_size) for _ in range(output_size)]
    
    def forward(self, inputs):
        return [p.forward(inputs) for p in self.perceptrons]
    
    def train(self, inputs, targets, LEARNING_RATE):
        for p, target in zip(self.perceptrons, targets):
            p.train(inputs, target, LEARNING_RATE)
            
# 다층 퍼셉트론 (MLP) 클래스
class MLP:
    def __init__(self, input_size, hidden_size, output_size):
        self.hidden = SLP(input_size, hidden_size)
        self.output = SLP(hidden_size, output_size)
    
    def forward(self, inputs):
        hidden_output = self.hidden.forward(inputs)
        return self.output.forward(hidden_output)
    
    def train(self, inputs, targets, LEARNING_RATE):
        # 순전파
        hidden_output = self.hidden.forward(inputs)
        __output = self.output.forward(hidden_output)
        
        # 출력층 델타 계산
        output_deltas = []
        for i in range(len(self.output.perceptrons)):
            delta = self.output.perceptrons[i].compute_delta(__output[i], targets[i])
            output_deltas.append(delta)
        
        # 은닉층 델타 계산
        hidden_goal = []
        hidden_deltas = []
        for i in range(len(self.hidden.perceptrons)):
            error = sum(self.output.perceptrons[j].weights[i] * output_deltas[j] for j in range(len(self.output.perceptrons)))
            target = error + hidden_output[i]
            delta = self.hidden.perceptrons[i].compute_delta(hidden_output[i], target)

            # 이건 다음 에러 계산에 쓸 거
            hidden_deltas.append(delta)
            
            # 이건 딸각에 넘겨줄 거
            hidden_goal.append(target)
            
        # 역전파 딸-깍
        self.output.train(hidden_output, targets, LEARNING_RATE)
        self.hidden.train(inputs, hidden_goal, LEARNING_RATE)
        
        return

# XOR 학습 함수
def generate_xor_data():
    return [
        ([0, 0], [0, 1]),
        ([0, 1], [1, 0]),
        ([1, 0], [1, 0]),
        ([1, 1], [0, 1])
    ]

def train_xor():
    DATA = generate_xor_data()
    mlp = MLP(input_size=2, hidden_size=18, output_size=2)
    EPOCHS = 10000
    LEARNING_RATE = 0.1
    
    for _ in range(EPOCHS):
        for inputs, targets in DATA:
            mlp.train(inputs, targets, LEARNING_RATE)
    
    # 학습된 결과 출력
    for inputs, targets in DATA:
        output = mlp.forward(inputs)
        print(f"입력: {inputs}, 예측: {output}, 실제: {targets}")

if __name__ == "__main__":
    train_xor()        
    </pre>
    </fieldset>

    <h2>21.Jan.2025</h2>
    <p>
        Most of header files are not written by doxygen.
        Adding @file on top would fix it. []
    </p>
</body>
</html>